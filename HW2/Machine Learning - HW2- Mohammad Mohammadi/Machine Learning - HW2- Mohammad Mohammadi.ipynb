{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eIfdgZzdwdti",
        "X5TyPFgPMa4a",
        "LD43kL3tSgz-",
        "JPt--IfXguTz",
        "9PTGrszCgwbu",
        "KEylV2nQmdq4",
        "wdCWyOxzNGVr",
        "OYi_INqrVMJB",
        "gpF6r3N0XWo5",
        "CPHH1oHGYvw6",
        "6igcvAQscIA5",
        "dW0qSf99gezo",
        "OrMsAClJiF3G",
        "wJY9o2XF-WIb",
        "Nuc4r-tpO4o6"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Course:** Machine Learning by Dr. Seyyed Salehi\n",
        "\n",
        "**Homework:** HW2\n",
        "\n",
        "**Name:** Mohammad Mohammadi\n",
        "\n",
        "**Student ID:** 402208592"
      ],
      "metadata": {
        "id": "9JbKLJNkfqlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1 - Valid Kernel\n",
        "According to the following information, determine whether each of the parts can be a kernel or not. If it is not a kernel, give a violation example and if it is a kernel, prove it.\n",
        "\n",
        "*   $a \\in \\mathbb{R}^+$\n",
        "*   $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^p$\n",
        "*   $K_1, K_2 \\text{ are kernels over } \\mathbb{R}^d \\times \\mathbb{R}^d$\n",
        "*   $K_3 \\text{ is a kernel over } \\mathbb{R}^p \\times \\mathbb{R}^p$\n",
        "\n",
        "(a) $K(x, z) = K_1(x, z) + K_2(x, z)$\n",
        "\n",
        "(b) $K(x, z) = aK_1(x, z)$\n",
        "\n",
        "(c) $K(x, z) = K_3(f(x), f(z))$"
      ],
      "metadata": {
        "id": "eIfdgZzdwdti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer 1\n",
        "\n"
      ],
      "metadata": {
        "id": "X5TyPFgPMa4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part a\n",
        "\n",
        "If $K_1$ and $K_2$ are kernels, they are symmetric and positive semi-definite. Therefore, for any set of points ${x_i}{i=1}^n$ and any set of real coefficients ${c_i}{i=1}^n$, we have:\n",
        "\n",
        "$\\sum_{i,j} c_i c_j K_1(x_i, x_j) \\geq 0 \\quad \\text{and} \\quad \\sum_{i,j} c_i c_j K_2(x_i, x_j) \\geq 0$.\n",
        "\n",
        "\n",
        "Adding these two inequalities, we get:\n",
        "\n",
        "$\\sum_{i,j} c_i c_j [K_1(x_i, x_j) + K_2(x_i, x_j)] \\geq 0$,\n",
        "\n",
        "which shows that $K(x, z) = K_1(x, z) + K_2(x, z)$ is positive semi-definite, and thus a kernel."
      ],
      "metadata": {
        "id": "mjNnEDiySJiq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part b\n",
        "\n",
        "**Under question's assumption of $a \\in \\mathbb{R}^+$:**\n",
        "\n",
        "\n",
        "Since $K_1$ is a kernel, it is positive semi-definite. For a positive scalar $a \\in \\mathbb{R}^+$ and any set of points ${x_i}{i=1}^n$ along with coefficients ${c_i}{i=1}^n$, we have:\n",
        "\n",
        "$\\sum_{i,j} c_i c_j aK_1(x_i, x_j) = a\\sum_{i,j} c_i c_j K_1(x_i, x_j) \\geq 0$,\n",
        "\n",
        "since $a > 0$ and $\\sum_{i,j} c_i c_j K_1(x_i, x_j) \\geq 0$ by the positive semi-definiteness of $K_1$.\n",
        "\n",
        "Based on the equation above $K(x, z) = aK_1(x, z)$ is positive semi-definite and symmetric, and thus a kernel."
      ],
      "metadata": {
        "id": "x65_aN-2S2TB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part c\n",
        "\n",
        "Based on the kernel trick we can say:\n",
        "\n",
        "Since $K_3$ is a kernel over $\\mathbb{R}^p \\times \\mathbb{R}^p$ and $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^p$ is a function mapping from the domain of our input space to the domain compatible with $K_3$, the composition $K_3(f(x), f(z))$ is also a kernel. For any set of points ${x_i}{i=1}^n$ and coefficients ${c_i}{i=1}^n$, we have:\n",
        "\n",
        "$\\sum_{i,j} c_i c_j K_3(f(x_i), f(x_j)) \\geq 0$,\n",
        "\n",
        "because $K_3$ is positive semi-definite by assumption.\n",
        "\n",
        "The mapping $f$ does not affect the positive semi-definiteness of $K_3$, hence, $K(x, z) = K_3(f(x), f(z))$ is a kernel.\n"
      ],
      "metadata": {
        "id": "gHKaQHxXT9zW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2 - Regularization\n",
        "\n",
        "Consider that we want to solve the binary classification problem shown in Figure 1 using a simple logistic regression model.\n",
        "\n",
        "$$P(y = 1\\mid x, w) = g(w_0 + w_1x_1 + w_2x_2) = \\frac{1}{1 + \\exp(-w_0 - w_1x_1 - w_2x_2)}$$\n",
        "\n",
        "As it is clear from the figure, the training data can be separated from each other with zero training error. Now we want to maximize the value of the following expression for large values of C to solve the classification problem.\n",
        "\n",
        "$$\\sum_{i=1}^{n} \\log P(y_i | x_i; \\omega_0, \\omega_1, \\omega_2) - C\\omega_j^2$$\n",
        "\n",
        "In this expression, $Cω_j^2$ is the regularization term that j∈{0,1,2}. In other words, only one of the parameters is regularized. According to the training data given in Figure 1, state that in each of the following situations, how will the training error compare with the simple logistic regression model? Give reasons for your answer.\n",
        "\n",
        "1.   Regularizing $ω_2$.\n",
        "2.   Regularizing $ω_1$.\n",
        "3.   Regularizing $ω_0$.\n",
        "\n",
        "Now consider that we want to regularize both parameters $ω_1$ and $ω_2$. In other words, we want to maximize the following expression in our model:\n",
        "\n",
        "$$\\sum_{i=1}^{n} \\log P(y_i | x_i; \\omega_0, \\omega_1, \\omega_2) - C(\\omega_1^2, \\omega_2^2)$$\n",
        "\n",
        "training data are the same as the data in Figure 1.\n",
        "\n",
        "4.  For large values of C, what values do we expect $\\omega_0$ to take? Give reasons for your answer.\n",
        "5.  This time, consider that we add some \"+\" data that are in class y = 1 to our training data. Assuming that the two classes are still fully separable, argue what values we expect $\\omega_0$ to take.\n",
        "\n",
        "Figure 1. \"+\" data belong to class y = 1, and \"O\" data belong to class y = 0."
      ],
      "metadata": {
        "id": "t6CKfFyQU1zQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer"
      ],
      "metadata": {
        "id": "Hj72CnKpbkMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1\n",
        "\n",
        "The feature $x_2$ separates the data very well, as shown in the figure. Regularizing $ω_2$ means that we are penalizing the logistic regression model for giving too much weight to $x_2$. If $C$ is very large, the regularization term $Cω_2^2$ will dominate, and the model will try to reduce $ω_2$ to minimize the penalty. However, since $x_2$ is a good separator, minimizing $ω_2$ would likely increase the training error compared to a model without regularization because the model would not leverage the full separating power of $x_2$."
      ],
      "metadata": {
        "id": "QqErxqC4bl43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2\n",
        "\n",
        "Regularizing $ω_1$ should not significantly affect the separation because $x_1$ does not separate the classes well; the data for both classes are mixed along the $x_1$ axis. The impact on training error would likely be less for regularizing $ω_1$ compared to regularizing $ω_2$."
      ],
      "metadata": {
        "id": "0qwnBmxQf38B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3\n",
        "\n",
        "Regularizing the bias term $ω_0$ usually makes less sense in the context of logistic regression, as it shifts the decision boundary away from the origin but does not control the shape of the boundary. Regularizing $ω_0$ would affect the position of the decision boundary, possibly increasing the training error if the boundary needs to be away from the origin to separate the classes effectively (Partialy similar to our case).\n",
        "\n",
        "However, since the data are linearly separable, it's possible to find a boundary that separates the classes with zero training error without relying on the bias term to be large."
      ],
      "metadata": {
        "id": "Wpdat2hchFT_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 4\n",
        "\n",
        "With both $ω_1$ and $ω_2$ being regularized, $ω_0$ would be the only term not penalized for being large. Hence, the optimization would likely result in $ω_0$ being larger relative to the other weights to achieve the best separation without incurring a penalty. Since $ω_0$ adjusts the threshold level for classification, it will make up for smaller values of $ω_1$ and $ω_2$ due to the high penalty on these parameters."
      ],
      "metadata": {
        "id": "J6DmzZMGiMGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 5\n",
        "\n",
        "If additional \"+\" data from class $y=1$ are added and the classes are still fully separable, $ω_0$ should be adjusted to account for the new data placement.\n",
        "\n",
        "Depending on where the new \"+\" data are placed, $ω_0$ could either increase or decrease to maintain zero training error.\n",
        "\n",
        "If the \"+\" data are placed farther away from the origin along the direction where $x_2$ is already providing good separation, $ω_0$ would not need to change significantly.\n",
        "\n",
        "If they are placed closer to the decision boundary, $ω_0$ might need to increase to push the decision boundary further from the origin to maintain separation."
      ],
      "metadata": {
        "id": "G7Zdr9G0jMLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3 - Gaussian Kernel and Valid Kernel\n",
        "\n",
        "(a) Show that Gaussian Kernel can be written as inner product of feature vectors with infinite dimenssions:\n",
        "\n",
        "$$k(\\mathbf{x}, \\mathbf{x'}) = e^{-\\frac{\\|\\mathbf{x} - \\mathbf{x'}\\|^2}{2\\sigma^2}}$$\n",
        "\n",
        "(b) Assume that $A \\in \\mathbb{R}^{p \\times p}$ is a symmetric and positive semi-difinite matrix. Prove that below is a valid kernel function:\n",
        "\n",
        "$$k(\\mathbf{x}, \\mathbf{y}) = \\mathbf{x}^\\mathrm{T}A\\mathbf{y}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "f-7wzE8JkHaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer"
      ],
      "metadata": {
        "id": "2T50xWbnMI55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part a\n",
        "\n",
        "The Gaussian Kernel by definition is defined as:\n",
        "\n",
        "$$k(\\mathbf{x}, \\mathbf{x'}) = e^{-\\frac{\\|\\mathbf{x} - \\mathbf{x'}\\|^2}{2\\sigma^2}}$$\n",
        "\n",
        "\n",
        "This kernel function can indeed be represented as an infinite-dimensional inner product, using Mercer's theorem.\n",
        "\n",
        "The idea is based on the fact that the Gaussian kernel is a positive semi-definite function and can be expanded into a Taylor series.\n",
        "\n",
        "The expansion of the exponential function in the Gaussian kernel corresponds to an inner product in an infinite-dimensional space.\n",
        "\n",
        "The Taylor expansion of the exponential function is as:\n",
        "\n",
        "$$e^z = \\sum_{n=0}^{\\infty} \\frac{z^n}{n!}$$\n",
        "\n",
        "So the exponential in the Gaussian kernel can be expanded as:\n",
        "\n",
        "$$e^{-\\frac{\\|\\mathbf{x} - \\mathbf{x'}\\|^2}{2\\sigma^2}} = e^{-\\frac{\\mathbf{x}^2}{2\\sigma^2}} \\cdot e^{-\\frac{\\mathbf{x'}^2}{2\\sigma^2}} \\cdot e^{\\frac{\\mathbf{x} \\cdot \\mathbf{x'}}{\\sigma^2}}$$\n",
        "\n",
        "Now we can expand $e^{\\frac{\\mathbf{x} \\cdot \\mathbf{x'}}{\\sigma^2}}$ using the Taylor series expansion and consider the inner product $\\mathbf{x} \\cdot \\mathbf{x'}$ which we can write as a sum over the individual dimensions:\n",
        "\n",
        "$$e^{\\frac{\\mathbf{x} \\cdot \\mathbf{x'}}{\\sigma^2}} = \\sum_{n=0}^{\\infty} \\frac{(\\mathbf{x} \\cdot \\mathbf{x'} / \\sigma^2)^n}{n!}$$\n",
        "\n",
        "Each term of this series can be thought of as an inner product of some feature mapping $ϕ(x)$ in a higher (possibly infinite) dimensional space.\n"
      ],
      "metadata": {
        "id": "GAxuY7_yMLBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part b\n",
        "\n",
        "In order to prove that a function $k(x,y)=x^TAy$ is a valid kernel function, we need to show that it satisfies Mercer's condition, which states that the kernel must be **symmetric** and **positive semi-definite**.\n",
        "\n",
        "**For Symmetry:**\n",
        "\n",
        "Since A is symmetric and positive semi-definite,  $A^T=A$ due to its symmetry, so we have:\n",
        "\n",
        "$$k(\\mathbf{x}, \\mathbf{y}) = \\mathbf{x}^\\mathrm{T}A\\mathbf{y} = \\mathbf{y}^\\mathrm{T}A\\mathbf{x} = k(\\mathbf{y}, \\mathbf{x})$$\n",
        "\n",
        "Which states that the kernel is symmetric.\n",
        "\n",
        "**For Positive Semi-Definiteness:**\n",
        "\n",
        "For any set of points $\\{x_i\\}_{i=1}^n$ and any set of coefficients $\\{c_i\\}_{i=1}^n$, we must have:\n",
        "\n",
        "$$\\sum_{i,j} c_i c_j k(\\mathbf{x}_i, \\mathbf{x}_j) = \\sum_{i,j} c_i c_j \\mathbf{x}_i^\\mathrm{T}A\\mathbf{x}_j \\geq 0$$\n",
        "\n",
        "We can express this as a matrix operation:\n",
        "\n",
        "$$\\mathbf{c}^\\mathrm{T}K\\mathbf{c} = \\mathbf{c}^\\mathrm{T}X^\\mathrm{T}AX\\mathbf{c}$$\n",
        "\n",
        "where $X$ is the matrix whose columns are the vectors $x_i$, $K$ is the kernel matrix with entries $K_{ij}=K(x_i, x_j)$ and $\\mathbf{c}$ is the vector of coefficients $c_i$.\n",
        "\n",
        "Since $A$ is positive semi-definite, the product $X^TAX$ is also positive semi-definite, which means $\\mathbf{c}^TK\\mathbf{c} \\geq 0$ for any $\\mathbf{c}$, and thus, $K$ is positive semi-definite.\n",
        "\n",
        "Hence $k(\\mathbf{x},\\mathbf{y})=\\mathbf{x}^TA\\mathbf{y}$ is a valid kernel function."
      ],
      "metadata": {
        "id": "wAF_k4GkVDgo"
      }
    }
  ]
}